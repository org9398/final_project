---
title: "Data Science for Public Policy"
subtitle: "Final Project"
author: "Olivia Gomez, Alicia Helfrich, Allie Gleich, Edward Malcolm"
execute:
  warning: false
format:
  html:
    embed-resources: true
---

## Final Project

1\. Background and Literature Review: Describes your research question, why the question\
is important to governance, policy, or social sciences, and a brief review of the literature\
that is relevant to your research question. Note that it will likely not be feasible to\
conduct a thorough literature review for this project, but I expect you to show that you\
have at least engaged with the existing research in your focus area.

\
2. Data Sources: This section should describe each of your data sources that you are using\
for your analysis and include your code to access and read in the data. If the data access\
includes any manual steps that are not captured in the code (e.g. downloading files in a\
point-and-click manner) please document those steps here.

**Current Population Survey - 2022**

We are utilizing the Current Population Survey from 2022 as a major source of data for our analysis. This survey is conducted each year to capture statistical information on the population. The units of measurement are individual adults. The survey contains information on immigration demographic characteristics in addition to health insurance status. Below, we will utilize unsupervised machine learning techniques, specifically dimension reduction, to analyze the relationship between health and demographic characteristics and estimate a model, given that the survey has many correlated predictors. We will also use the data in a supervised machine learning capacity to predict health coverage status based on several demographic predictors.

We used the IPUMS to download the 2022 CPS data. We read in the data using the ipumsr package.

```{r}
library(ipumsr)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(corrplot)
library(VGAM)
library(rpart)
library(rpart.plot)
library(hardhat)
library(ranger)
library(recipes)
library(srvyr)
library(haven)
library(survey)
library(tidyclust)
library(GGally)
library(patchwork)
```

```{r}

#Loading in the CPS 2022 data from IPUMS
ddi <- read_ipums_ddi("data/cps_00002.xml")
cps <- read_ipums_micro(ddi)

```

\
3. Data Wrangling and Exploratory Data Analysis: This section should include your code\
to perform any data cleaning and new variable creation. You should also thoroughly\
explore your data, including assessing the presence of outliers/unexpected values and\
identifying and appropriately addressing missingness in any key variables.

```{r}

# Data Cleaning and Wrangling for CPS

cps <- cps %>%
  janitor::clean_names() %>%
  select(-year, -month, -mbpl, -fbpl, -asian)

cps_na <- is.na(cps)

missing_counts <- colSums(cps_na)

print(missing_counts[missing_counts > 0])

# missing values are ones that are not part of the ASEC supplemental, so we filter those observations out

cps <- cps %>%
  filter(!is.na(asecwth)) %>%
  select(-hwtfinl, -wtfinl)

# create variable for health insurance status 

cps <- cps %>%
  mutate(healthinsu = case_when(
    himcaidnw == 1 ~ "medicaid", 
    himcarenw == 1 ~ "medicare",
    prvtcovnw == 1 ~ "private",
    grpcovnw == 1 ~ "employment based",
    dpcovnw == 1 ~ "direct purchase",
    mrkcovnw == 1 ~ "marketplace",
    trccovnw == 1 ~ "tricare", 
    inhcovnw == 1 ~ "indian health service",
    TRUE ~ NA_character_  
  )) 

cps <- cps %>%
  mutate(healthinsu = factor(healthinsu, levels = c(
    "medicaid", "medicare", "private", "employment based", 
    "direct purchase", "marketplace", "tricare", 
    "indian health service"
  ), labels = c(
    "Medicaid", "Medicare", "Private", "Employment Based", 
    "Direct Purchase", "Marketplace", "Tricare", 
    "Indian Health Service"
  )))

cps <- cps %>%
  filter(!is.na(anycovnw)) %>%
  select(-himcaidnw, -himcarenw, -prvtcovnw, -grpcovnw, -dpcovnw, -mrkcovnw, -trccovnw, -inhcovnw) %>%
  mutate(healthinsu = as_factor(healthinsu))
  

cps_svy <- as_survey_design(.data = cps, weights = asecwt) 

cps <- cps %>%
  mutate_all(as.numeric)

cps <- cps %>%
  select(-asecflag)
```

\
4. Data Analysis: This section should include the code to conduct analysis to answer your\
question of interest. It should include writing explaining why the tools you selected are\
a good fit for the research question and a justification of key analytic decisions. If you\
are using machine learning methods, this section should include model evaluation using\
the methods discussed in class.

Part A) Unsupervised Machine Learning using data from the CPS

```{r}

# correlation plot of data to highlight multicollinearity 

correlation <- cps %>%
  select(-serial, -cpsid, -asecwth, -pernum, -cpsidp, -cpsidv, -asecwt) %>%
  cor()

corrplot(correlation, method = 'color')

# creating PCA recipe 
rec_pca <- recipe(
  formula = ~ .,
  data = cps) %>%
  update_role(serial, cpsid, asecwth, pernum, cpsidp, cpsidv, asecwt, new_role = "Id") %>%
  step_scale(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = 5) %>%
  prep(data = cps) 

# apply estimated loadings to original data
cps_pcs <- rec_pca %>%
  bake(new_data = cps)

# Extract Loadings
coef <- rec_pca %>%
  tidy(type = "coef", number = 2)

# Extract variance explained
variance <- rec_pca %>%
  tidy(type = "variance", number = 2) %>%
  filter(terms == "variance") %>%
  mutate(pct_var = value/sum(value)) %>%
  slice_head(n = 5) 

print(variance)

cps_pcs_all <- 
  bind_cols(cps, cps_pcs)

# making scatterplot to visualize data

plot1 <- cps_pcs_all %>%
  ggplot(mapping = aes(x = PC1, y = PC2, color = citizen)) +
  geom_point() + 
  labs(title = "Scatterplot of PC1 and PC2 by Citizenship",
       x = "Principal Component 1",
       y = "Principal Component 2") + 
  theme(plot.title = element_text(size = 12))

plot2 <- cps_pcs_all %>%
  ggplot(mapping = aes(x = PC1, y = PC2, color = healthinsu)) +
  geom_point() + 
  labs(title = "Scatterplot of PC1 and PC2 by Health Insurance Status",
       x = "Principal Component 1",
       y = "Principal Component 2") + 
  theme(plot.title = element_text(size = 12))

plot1 + plot2

# Creating a kmeans model

# set up cross-validation
folds <- vfold_cv(cps, v = 5)

kmeans_rec <- recipe(
formula = ~ .,
data = cps
) %>%
  update_role(serial, cpsid, asecwth, pernum, cpsidp, cpsidv, asecwt, new_role = "Id") %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_scale(all_predictors()) %>%
  step_center(all_predictors()) 

kmeans_numeric <- kmeans_rec %>%
prep() %>%
bake(new_data = cps)

# hyperparameter tune first to obtain optimal number of clusters

# create a kmeans model object four clusters
kmeans_spec <- k_means(
num_clusters = 4 
) %>%
set_engine(
"stats",
nstart = 100, iter.max = 1000
)

# create a workflow
kmeans_wflow <- workflow(
preprocessor = kmeans_rec,
spec = kmeans_spec
)
# set a seed because the clusters are not deterministic
set.seed(20200205)

# fit the model
fit <- kmeans_wflow %>%
fit(data = cps)

# view the model results
tidy(fit) %>%
knitr::kable(digits = 2)
```

\
5. Discussion of Results: This section should include the interpretation of your results.\
Please discuss what your results suggest about the answer to your research question\
of interest. Please also discuss any limitations of your analysis and areas for potential\
future research.
