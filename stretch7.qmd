---
title: "Data Science for Public Policy"
subtitle: "Stretch Exercise - Assignment 7"
author: "Olivia Gomez"
execute:
  warning: false
format:
  html:
    embed-resources: true
---

## Stretch Exercise - Assignment 7

I am using data from the National Health Interview Survey. This survey is conducted each year to capture statistical information on the amount, distribution, and effects of illness and disability in the United States. The units of measurement are individual adults. I will be utilizing a supervised machine learning model to predict health status (a measure of one's self-reported health on a scale of 1 to 5) based on several different factors.

1.  Set Up

```{r}

library(ipumsr)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(corrplot)
library(VGAM)
library(rpart)
library(rpart.plot)
library(hardhat)
library(ranger)

# Loading data

ddi <- read_ipums_ddi("nhis_00001.xml")
data <- read_ipums_micro(ddi)

# Data Cleaning and Subsetting

data_clean <- data %>%
  janitor::clean_names() %>%
  mutate(usborn_binary = if_else(usborn %in% c(10, 11, 12), "1", "2")) %>%
  filter(!(health %in% c(7, 8, 9)), !(yrsinusg %in% c(0, 7, 8, 9))) %>%
  select(!cpi2009 & !year)
  

data_clean$health <- factor(data_clean$health, levels = 1:5)


# Splitting data into training and testing data 

split <- initial_split(data = data_clean, prop = 0.8)

data_train <- training(x = split)
data_test <- testing(x = split)

# Exploratory Data Analysis and Data Cleaning 

data_train %>%
  filter(usborn_binary == "1") %>%
  group_by(health, yrsinusg) %>%
  summarise(count = n()) %>%
  ggplot() +
  geom_col(mapping = aes(x = yrsinusg, y = count, fill = factor(health))) +
  labs(title = "Health Status by Years in the US",
       x = "Years in the US",
       y = "Count") +
  theme_minimal()

data_train %>%
  filter(usborn_binary == "1") %>%
  group_by(health, yrsinusg) %>%
  summarise(count = n()) %>%
  ggplot(mapping = aes(x = yrsinusg, y = health, fill = count)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") 

```

I will use accuracy, as my models will be using the classification metric. The cost of an error would be falsely predicting one's health status. Predicting a health status too high could indicate a inaccurately healthier population, and vice versa for predicting too low. Both errors can have implications on policy interventions. However, the difference in predicting between a 2 or a 3, for example, will have less implications and less of a cost for the model.

2\. Come up with Models

Model Specification 1

*Predicting health status* *using demographic variables (ethnicity, race, age, citizenship status, etc.) in a decision tree model.*

```{r}

# Set up v-fold cross-validation with 10 folds.

folds <- vfold_cv(data = data_train, v = 10)

# Create a recipe

recipe <- 
  recipe(health ~ age + sex + hisprace + yrsinusg + citizen, data = data_train) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_scale(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_center(all_predictors()) 

bake(prep(recipe, training = data_train), new_data = data_train)

# model specification 1

decision_tree_mod <- decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

# model workflow 1

decision_tree_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(decision_tree_mod)

```

Model Specification 2

*Predicting health status* *using demographic variables (ethnicity, race, age, citizenship status, etc.) in a decision tree model in a KNN model.*

```{r}

# model specification 2

knn_mod <-
  nearest_neighbor(neighbors = tune()) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "classification")

knn_grid <- grid_regular(neighbors(range = c(1, 49)), levels = 8)

# model workflow 2

knn_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(knn_mod)

```

Model Specification 3

*Predicting health status* *using demographic variables (ethnicity, race, age, citizenship status, etc.) in a Random Forest model.*

```{r}

# model specification 3

rf_mod <- rand_forest(
  trees = 50,    
  min_n = 5) %>%
  set_mode("classification") %>%
  set_engine("ranger")

# create a workflow 3

rf_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(rf_mod)


```

3\. Estimation

```{r}

# fitting random forest model to training data 

rf_fit <- fit_resamples(
  rf_wf,
  resamples = folds,
  control = control_resamples(save_pred = TRUE),
  metrics = metric_set(accuracy))

# extract metrics to interpret

metrics <- rf_fit %>%
  collect_metrics(summarize = TRUE)

print(metrics)

```

4.  Interpretation

Per the accuracy rate, the model is only accurate in predicting health status 37% of the time. This model is thus not very useful at predicting health status. Implementing this model only utilizing a few immigration-related demographic factors could be difficult and inaccurate, and highlights the complexity involved with one's self-reported health status. However, a successful model could be helpful, as it could inform policy leaders on how to address low self-reported health.

One feature engineering choice I may try next to improve upon the result are creating interaction terms. It is clear that several of the variables interact, like citizenship status and years in the US. The more time someone spends in the US, the more likely they are to be a citizen. I may try to create an interaction term to combine the effects of these two variables into one to account for that combined effect. Additionally, another feature engineering tactic I would use is Synthetic Minority Over-sampling Technique (SMOTE). Not all categories of health status have equal frequency in the data set, as evidenced through the data visualizations I created during my exploratory data analysis. By using SMOTE, I could balance the classes.

In terms of modeling choices, I may try to utilize ordinal regression methods instead of classification, which may help improve accuracy of the predictions. Additionally, I will attempt to use a decision tree or KNN which may prove more accurate.
